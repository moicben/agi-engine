{
  "content": "{\n  \"stage\": \"Plan\",\n  \"iteration\": \"1\",\n  \"tasks\": [\n    {\n      \"id\": \"T1\",\n      \"name\": \"Validate and normalize question\",\n      \"objective\": \"Ensure the user query is well-formed, unambiguous, and enriched with any required context before retrieval\",\n      \"inputs\": {\n        \"user_query\": \"string (raw question text)\",\n        \"user_context\": \"optional string (previous turns, domain, constraints)\"\n      },\n      \"outputs\": {\n        \"validated_query\": \"string (normalized question)\",\n        \"clarification_needed\": \"boolean\",\n        \"clarification_prompt\": \"string (if clarification_needed=true)\"\n      },\n      \"actions\": [\n        \"If user_query is empty or whitespace -> set clarification_needed=true and clarification_prompt='Please provide a question.'\",\n        \"Normalize whitespace, punctuation, and convert to canonical casing for processing.\",\n        \"Detect question type (fact, explanation, instruction, multi-part) and required scope (timeframe, domain).\",\n        \"If detected ambiguity or missing scope (e.g., unspecified timeframe or entity) -> set clarification_needed=true and generate a specific clarification_prompt listing missing fields.\",\n        \"If no ambiguity -> set validated_query to normalized form and clarification_needed=false.\"\n      ],\n      \"dependencies\": [],\n      \"acceptance\": [\n        \"validated_query is non-empty when clarification_needed=false\",\n        \"If clarification_needed=true then clarification_prompt is a specific question asking for only missing info\"\n      ],\n      \"risks\": [\n        \"Overlooking implicit constraints leading to wrong retrieval\",\n        \"False positives for ambiguity causing unnecessary clarifications\"\n      ],\n      \"mitigations\": [\n        \"Use conservative ambiguity detection: require explicit missing critical fields (timeframe, entity) before asking\",\n        \"If unsure, include both likely interpretations in downstream retrieval metadata rather than blocking\"\n      ]\n    },\n    {\n      \"id\": \"T2\",\n      \"name\": \"Retrieve and filter evidence\",\n      \"objective\": \"Deterministically collect relevant, high-quality evidence and sources to support the answer\",\n      \"inputs\": {\n        \"validated_query\": \"string (from T1)\",\n        \"retrieval_params\": {\n          \"sources\": \"list (e.g., local KB, indexed docs, web) - default: local KB then web\",\n          \"max_results\": \"integer (default 10)\",\n          \"relevance_threshold\": \"float (0.6 default)\"\n        }\n      },\n      \"outputs\": {\n        \"evidence_set\": \"array of {id, source, excerpt, relevance_score, timestamp}\",\n        \"retrieval_log\": \"object (queries used, filters applied)\"\n      },\n      \"actions\": [\n        \"Formulate deterministic search queries from validated_query (one canonical query + up to 2 synonym-expanded queries).\",\n        \"Search local knowledge base/index; if results < 1 or relevance < relevance_threshold, search web sources.\",\n        \"Rank results by relevance_score (compute using deterministic scoring function) and filter to entries with relevance_score >= relevance_threshold or top N results if none meet threshold.\",\n        \"For each retained result, extract a short excerpt supporting potential claims and record source metadata and timestamp.\",\n        \"Produce retrieval_log listing queries, sources searched, and filtering decisions.\"\n      ],\n      \"dependencies\": [\n        \"T1\"\n      ],\n      \"acceptance\": [\n        \"evidence_set contains >=1 item with relevance_score >= relevance_threshold OR top N results present when none"
}